# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The given dataset is bankmarketing_train.csv. This is a sample dataset provided by Microsoft. According to their documentation, the y column indicates if a customer subscribed to a fixed term deposit. The dataset contains information about customers - their job, marital status, education, loan, etc.
This is a classification problem, where we tried to predict whether a customer is subscribed to a fixed-term deposit, or not. 

I have tried out to analyze the cleaned, as well as the raw data. After trying out different models, the best accuracy reached for different approaches was:

| Data   | Approach | Accuracy |
| ------------- | ---------|------------- |
| Cleaned data | SKLearn - Logistic Regression  | 0.91181  |
| Original data | AutoML - VotingEnsemble  | 0.91697  |
| Cleaned data | AutoML - VotingEnsemble  | 0.91643  |

As it can be seen, all three approaches have found best models with similar results. 

## Scikit-learn Pipeline
The aim of this project was to create the following pipeline: 

![Alt text](screenshots/0.%20creating-and-optimizing-an-ml-pipeline.png?raw=true "Optional Title")

As can be seen at the first glance, the whole project consists of three major elements: 
- train.py
- Jupyter notebook
- Research report

Research report is the document you are reading now. The other two had to be uploaded to the Azure environment, to the Notebook first: 

![Alt text](screenshots/1.%20Uploaded%20files.PNG?raw=true "Optional Title")

In order to make the scripts work, a compute instance is needed. For that, a Standard_D2_v2 machine was selected:

![Alt text](screenshots/2.%20Creating%20Compute.PNG?raw=true "Optional Title")

This instance was named as "Udacity-Lab1". 

### train.py
The whole ML pipeline begins with the train.py script, which is pre-given. The main aim of this script is data preparation. There, the dataset had to be uploaded, cleaned and split into training and testing set. In order to read the data, the TabularDatasetFactory was used. 

```
datastore_path = "https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv"
ds = TabularDatasetFactory.from_delimited_files(path=datastore_path)
```

The Logistic Regression accepts two arguments: regularization strength and maximum number of iterations to converge. Per default, they are equal to 1.0 and 100 respectively. 
The train.py is then called from the Jupyter notebook. 

### Jupyter notebook
The Jupyter notebook has several logical parts. At first, the new experiment must be created. The one here was called "quick-starts-experiment". 

![Alt text](screenshots/4.%20Workspace%20found.PNG?raw=true "Optional Title")



**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
